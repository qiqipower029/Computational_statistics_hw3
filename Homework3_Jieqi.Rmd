---
title: "Homework3"
author: "Jieqi Tu"
date: "10/7/2020"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Question 1
```{r define function from professor}
# Define the function written by Dr. Demirtas

############################################################
#EM algorithm for univariate normal MAR example
#Written by Hakan Demirtas
############################################################
# Required arguments:
# y= incomplete univariate data, with missing values
# denoted by NA.
# start=starting values for parameters. This should be a
# list with two components: mu and sigma
#
# Optional arguments:
# maxits=maximum number of iterations
# eps=convergence criterion
#
# Output:
# This function returns a list with the following components:
# theta=final parameter estimates
# iter=how many iterations were performed
# cvgd=logical value indicating whether it converged or not.
# loglik= vector of length ``iter'' giving the observed-data
# log-likelihood at each iteration
##############################################################

univariate.normal.em.demirtas<-function(y, start, maxits=500, eps=0.000001){
  newmu<-start$mu ; newsigma2<-start$sigma2 ; n<-length(y)
  w<-!is.na(y) ; yobs<-y[w] ; nobs<-length(yobs) ; nmis<-n-nobs
  loglik<-numeric(maxits) ; iter<-0 ; cvgd<-F
  cat(paste("Performing iterations of EM...","\n"))
  
  while ((iter<maxits)&(!cvgd)){
  iter<-iter+1 ; mu<-newmu ; sigma2<-newsigma2
  # EVALUATE OBSERVED-DATA-LIKELIHOOD
  ll<--(nobs/2)*log(sigma2)-1/(2*sigma2)*sum((yobs-mu)^2)
  loglik[iter]<-ll
  # E-STEP
  ET1<-sum(yobs)+nmis*mu
  ET2<-sum(yobs^2)+nmis*(sigma2+mu^2)
  # M-STEP
  newmu<-ET1/n
  newsigma2<-(ET2/n)-newmu^2
  # ASSESS CONVERGENCE
  cvgd<-(abs(newmu-mu)<=eps*abs(mu)) & (abs(newsigma2-sigma2)<=eps*abs(sigma2))}
  
  cat(paste("Done!","\n"))
  theta<-list(mu=newmu, sigma2=newsigma2)
  loglik<-loglik[1:iter]
  result<-list(theta=theta, iter=iter, cvgd=cvgd,loglik=loglik)
  result}

```

```{r question 1}
# Import data
data = c(50.1, 39.8, 46.4, NA, NA, 31.2, 50.0, 49.6, 48.7, 49.7, 53.4, 62.1, NA)

# Convergence criteria
eps = c(0.001, 0.0001, 0.00001, 0.000001)

# Calculate the sample mean and sample variance (only observed data)
mu = mean(data, na.rm = T); mu
sigma2 = sum((data[!is.na(data)] - mu)^2)/length(data[!is.na(data)]); sigma2

# Use the result to be the starting values
start1 = list(mu = 48.1, sigma2 = 59.426)

# Try resonable starting value
start2 = list(mu = 50, sigma2 = 60)

# Try some crazy starting values
start3 = list(mu = 0, sigma2 = 10)
start4 = list(mu = 1000, sigma2 = 1000)
start5 = list(mu = 10000, sigma2 = 21000)

# Create variables to store results
mu_result = numeric(20)
sigma2_result = numeric(20)
iteration = numeric(20)
log_like = numeric(20)

# Start iterations and try 4 different convergence criteria 
for (i in 1:4) {
  # At each iteration, try different starting values
  EM1 = univariate.normal.em.demirtas(data, start1, eps = eps[i])
  mu_result[i] = EM1$theta$mu
  sigma2_result[i] = EM1$theta$sigma2
  iteration[i] = EM1$iter
  log_like[i] = EM1$loglik[EM1$iter]
  
  
  EM2 = univariate.normal.em.demirtas(data, start2, eps = eps[i])
  mu_result[i+4] = EM2$theta$mu
  sigma2_result[i+4] = EM2$theta$sigma2
  iteration[i+4] = EM2$iter
  log_like[i+4] = EM2$loglik[EM2$iter]
  
  
  EM3 = univariate.normal.em.demirtas(data, start3, eps = eps[i])
  mu_result[i+8] = EM3$theta$mu
  sigma2_result[i+8] = EM3$theta$sigma2
  iteration[i+8] = EM3$iter
  log_like[i+8] = EM3$loglik[EM3$iter]
  
  
  EM4 = univariate.normal.em.demirtas(data, start4, eps = eps[i])
  mu_result[i+12] = EM4$theta$mu
  sigma2_result[i+12] = EM4$theta$sigma2
  iteration[i+12] = EM4$iter
  log_like[i+12] = EM4$loglik[EM4$iter]
  
  
  EM5 = univariate.normal.em.demirtas(data, start5, eps = eps[i])
  mu_result[i+16] = EM5$theta$mu
  sigma2_result[i+16] = EM5$theta$sigma2
  iteration[i+16] = EM5$iter
  log_like[i+16] = EM5$loglik[EM5$iter]
}

# Sort the results
convergence_criteria = rep(eps, 5)
start_mu = c(rep(start1$mu, 4), rep(start2$mu, 4), rep(start3$mu, 4), rep(start4$mu, 4), rep(start5$mu, 4))
start_sigma2 = c(rep(start1$sigma2, 4), rep(start2$sigma2, 4), rep(start3$sigma2, 4), rep(start4$sigma2, 4), rep(start5$sigma2, 4))
result_table = cbind(convergence_criteria, start_mu, start_sigma2, mu_result, sigma2_result, iteration, log_like) %>% as.data.frame()

result_table %>% knitr::kable()
```

From the result, we could see that, the estimated $\hat\mu$ are almost the same no matter what starting values and convergence criteria we choose. However, the value of $\hat{\sigma^2}$ are affected by the convergence criteria. When we choose the sample mean and sample variance of observed data as the starting values, the iteration is only 1. But other starting values lead to more iterations, especially higher precision of convergence criteria. If we set the convergence criteria more precise, and give enough iterations, we can reach very similar (even identical) parameter estimates through EM.

Then we can try another dataset with the same mean and variance and the same sample size. Let's see whether EM would give us the same estimates.
```{r question 1 new dataset}
# Create a new dataset
set.seed(1029)
data_new = rnorm(13, mean = 48.1, sqrt(59.426))

# Randomly choose 3 data points to be missing
missing = sample(1:13, size = 3, replace = F); missing
data_new[missing] = NA; data_new

# Calculate the sample mean and sample variance from the new dataset
mu = mean(data_new, na.rm = T); mu
sigma2 = sum((data_new[!is.na(data_new)] - mu)^2)/length(data_new[!is.na(data_new)]); sigma2

# Use the new dataset to run EM method again
start_new = list(mu = mu, sigma2 = sigma2); start_new
start_new2 = list(mu = 0, sigma2 = 100); start_new2
univariate.normal.em.demirtas(data_new, start_new)
univariate.normal.em.demirtas(data_new, start_new2)
```

We could see that, if we try a sample that is derived from the population, the results could be different. So the EM method can provide estimates based on the sample instead of the true population. Also it is found that EM method will converge to the observed data sample mean and sample variance in univariate cases.

Let's try another dataset with higher portion of missing points. In this dataset, 6 points will be missing.
```{r question 1 another dataset}
# Create a new dataset
set.seed(1029)
data_new = rnorm(13, mean = 48.1, sqrt(59.426))

# Randomly choose 3 data points to be missing
missing = sample(1:13, size = 6, replace = F); missing
data_new[missing] = NA; data_new

# Calculate the sample mean and sample variance from the new dataset
mu = mean(data_new, na.rm = T); mu
sigma2 = sum((data_new[!is.na(data_new)] - mu)^2)/length(data_new[!is.na(data_new)]); sigma2

# Use the new dataset to run EM method again
start_new = list(mu = mu, sigma2 = sigma2); start_new
start_new2 = list(mu = 0, sigma2 = 100); start_new2

univariate.normal.em.demirtas(data_new, start_new)
univariate.normal.em.demirtas(data_new, start_new2)
```

From the result, we could see that, if the missing portion becomes larger, we need more iterations to get the convergence. Here we can also see that EM method leads us to the sample mean and sample variance of observed data.

Let's see how stable the estimation of EM method gives us.
```{r question 1 1000 loops}
mu_result = vector()
sigma2_result = vector()
set.seed(1029)
for (i in 1:1000) {
  data_new = rnorm(13, 48.1, sqrt(59.426))
  mis = sample(1:13, 3, replace = F)
  data_new[mis] = NA; data_new

  mu = mean(data_new, na.rm = T)
  sigma2 = sum((data_new[!is.na(data_new)] - mu)^2)/length(data_new[!is.na(data_new)])
  start_new = list(mu = mu, sigma2 = sigma2)
  result = univariate.normal.em.demirtas(data_new, start_new)
  theta = result$theta
  mu_result[i] = theta$mu
  sigma2_result[i] = theta$sigma2
}

mean(mu_result)
sd(mu_result)
mean(sigma2_result)
sd(sigma2_result)
```

From the result, we could see that, in EM method, $\hat{\sigma^2}$ has much larger variance than $\hat\mu$. This means that the result for estimation of mean is more stable.


## Question 2
Add two additional steps in the previous EM function and define the function to calculate elementwise convergence rate.
```{r question 2 functions}
# Add two additional steps in the previous EM function
univariate.normal.em.demirtas_new = function(y, start, maxits=500, eps=0.000001){
  newmu<-start$mu ; newsigma2<-start$sigma2 ; n<-length(y)
  w<-!is.na(y) ; yobs<-y[w] ; nobs<-length(yobs) ; nmis<-n-nobs
  loglik<-numeric(maxits) ; iter<-0 ; cvgd<-F
  mut = c()
  sigma2t = c()
  cat(paste("Performing iterations of EM...","\n"))
  
  while ((iter<maxits)&(!cvgd)){
  iter<-iter+1 ; mu<-newmu ; sigma2<-newsigma2
  # EVALUATE OBSERVED-DATA-LIKELIHOOD
  ll<--(nobs/2)*log(sigma2)-1/(2*sigma2)*sum((yobs-mu)^2)
  loglik[iter]<-ll
  # E-STEP
  ET1<-sum(yobs)+nmis*mu
  ET2<-sum(yobs^2)+nmis*(sigma2+mu^2)
  # M-STEP
  newmu<-ET1/n
  newsigma2<-(ET2/n)-newmu^2
  
  # Record parameter theta_t
  mut[iter] = newmu
  sigma2t[iter] = newsigma2
  
  # ASSESS CONVERGENCE
  cvgd<-(abs(newmu-mu)<=eps*abs(mu)) & (abs(newsigma2-sigma2)<=eps*abs(sigma2))}
  
  # Adding two additional steps
  for (i in 1:2) {
    iter = iter + 1
    mu = newmu
    sigma2 = newsigma2
    # EVALUATE OBSERVED-DATA-LIKELIHOOD
    ll<--(nobs/2)*log(sigma2)-1/(2*sigma2)*sum((yobs-mu)^2)
    loglik[iter]<-ll
    # E-STEP
    ET1<-sum(yobs)+nmis*mu
    ET2<-sum(yobs^2)+nmis*(sigma2+mu^2)
    # M-STEP
    newmu<-ET1/n
    newsigma2<-(ET2/n)-newmu^2
  
    # Record parameter theta_t
    mut[iter] = newmu
    sigma2t[iter] = newsigma2
  }
  cat(paste("Done!","\n"))
  theta<-list(mu=mut, sigma2=sigma2t)
  loglik<-loglik[1:iter]
  result<-list(theta=theta, iter=iter, cvgd=cvgd,loglik=loglik)
  result}

# Calculate the elementwise rate of convergence
# Because we added two steps, t-1 will be the step that we get the convergence
rate_cal = function(theta) {
  rate = c()
  t = length(theta)
  for (i in 2:t-1) {
    rate[i] = (theta[i+1]-theta[i])/(theta[i]-theta[i-1])
  }
  return(rate)
}
```

Then we create the dataset.
```{r question 2 dataset}
# Create the dataset, normally distributed with mean 50 and sd 10
set.seed(1029)
data_2 = rnorm(100, 50, 5)
mis = sample(1:100, size = 90, replace = F)
data_2[mis] = NA

# Calculate the sample mean and sample variance
mu = mean(data_2, na.rm = T); mu
sigma2 = sum((data_2[!is.na(data_2)] - mu)^2)/length(data_2[!is.na(data_2)]); sigma2

# Set starting values
start_value = list(mu = mu, sigma2 = 10); start_value

# Run EM function
result_question2 = univariate.normal.em.demirtas_new(data_2, start_value)
mu_result = result_question2$theta$mu
sigma2_result = result_question2$theta$sigma2

# Calculate the convergence rate for mu
rate_cal(mu_result)

# Calculate the convergence rate for sigma2
rate_cal(sigma2_result)
```

Since the $\hat\mu$ converges at the very beginning, the denominator becomes 0, the rate is not available in thie case. Also, the elementwise rate of convergence for $\hat{\sigma^2}$ is always 0.9, which is also the proportion of missing in the sample (90 out of 100 are missing). Let's try another pair of starting values that are not convergent at beginning.
```{r question 2 another starting values}
start_value = list(mu = 0, sigma2 = 0)

# Run EM function
result_question2 = univariate.normal.em.demirtas_new(data_2, start_value)
mu_result = result_question2$theta$mu
sigma2_result = result_question2$theta$sigma2

# Calculate the convergence rate for mu
rate_cal(mu_result)

# Calculate the convergence rate for sigma2
rate_cal(sigma2_result)
```

In this case, $\hat\mu$ has the elementwise rate of convergence at 0.9. Because the update of new $\hat{\sigma^2}$ is dependent on $\hat\mu$, the rate for $\hat{\sigma^2}$ is not 0.9 at the early phase of iterations. However, when $\hat\mu$ became stable, the rate for $\hat{\sigma^2}$ is 0.9. 

According to the paper written by Schafer in 1997, in most cases, the elementwise rate of convergence would estimate the largest eigenvalue of the ratio between the missing information to the complete data in the complete data information. Also this value could be affected by the starting values.

## Question 3
Import the dataset.
```{r question 3}
# Import dataset
mixture = read.table(file = "./mixture.dat")
data3 = t(mixture)
data3 = as.vector(data3)
```

Then define the functions provided.
```{r define functions for question 3}
###########################################################
# EM algorithm for two-part mixture of exponentials
# Written by Hakan Demirtas
# startval should be a list with three components:
# pi=mixing proportion
# lambda1=reciprocal mean of the first component
# lambda1=reciprocal mean of the second component
###########################################################
# The function below allows us that pi can be fixed
# to a specific value to perform constrained estimation.
em.exponential.mixture<-function(y, startval, maxits=10000, eps=0.00001, pi.fix){
  n<-length(y) ; newtheta<-startval
  if(!missing(pi.fix)) newtheta$pi<-pi.fix
  iter<-0
  converged<-F
  loglik<-numeric(maxits)
  while((!converged)&(iter<maxits)){
    iter<-iter+1
    theta<-newtheta
    
    # DO E-STEP
    pix<-theta$pi
    lambda1<-theta$lambda1
    lambda2<-theta$lambda2
    num<-pix*lambda1*exp(-lambda1*y)
    denom<-num+(1-pix)*lambda2*exp(-lambda2*y)
    delta<-num/denom # parameter for the Bernoulli distribution
    T1<-sum(delta) ; T2<-sum(delta*y) ; T3<-sum(y)
    loglik[iter]<-sum(log(denom))
    
    # DO M-STEP
    if (missing(pi.fix)) pix<-T1/n
    lambda1<-T1/T2
    lambda2<-(n-T1)/(T3-T2)
    newtheta<-list(pix=pix, lambda1=lambda1, lambda2=lambda2)
    
    # ASSESS CONVERGENCE
    old<-c(theta$pi, theta$lambda1, theta$lambda2)
    new<-c(newtheta$pi, newtheta$lambda1, newtheta$lambda2)
    converged<-all(abs(old-new)<=eps*abs(old))}
    loglik<-loglik[1:iter]
    list(theta=newtheta,iter=iter, converged=converged, loglik=loglik)}
```

Then we can try some starting values. Here we want to fix the $\hat\pi$ at 0.45.
```{r question 3 try starting values}
start1 = list(pi = 0.3, lambda1 = 1, lambda2 = 0.5)
start2 = list(pi = 0.7, lambda1 = 0.5, lambda2 = 1)
em.exponential.mixture(data3, start1, eps = 0.0000001)
em.exponential.mixture(data3, start2, eps = 0.0000001)
```

From the result, we could see that, if we interchange the $\pi$ and $1-\pi$, and $\lambda_1$ and $\lambda_2$, the likelihood would be the same. Then we might not be able to know how the true distribution it is.

We can visualize the situation by plotting the log likelihood function. 
```{r question 3 plot the log likelihood, message=FALSE}
pi_range = seq(0.01, 0.99, 0.001)
start3 = list(pi = 0.5, lambda1 = 0.75, lambda2 = 0.75)

logL1 = numeric(length(pi_range))
logL2 = numeric(length(pi_range))
logL3 = numeric(length(pi_range))
for (i in 1:length(pi_range)) {
  result1 = em.exponential.mixture(data3, start1, eps = 0.000001, pi.fix = pi_range[i])
  result2 = em.exponential.mixture(data3, start2, eps = 0.000001, pi.fix = pi_range[i])
  result3 = em.exponential.mixture(data3, start3, eps = 0.000001, pi.fix = pi_range[i])
  conv_n1 = result1$iter
  conv_n2 = result2$iter
  conv_n3 = result3$iter
  logL1[i] = result1[[4]][conv_n1]
  logL2[i] = result2[[4]][conv_n2]
  logL3[i] = result3[[4]][conv_n3]
}

plot_table = cbind(pi_range, logL1, logL2, logL3) %>% as.data.frame()
plot_table %>% 
  ggplot(aes(x = pi_range)) + 
  geom_line(aes(y = logL1, colour = "pi")) +
  geom_line(aes(y = logL2, colour = "1-pi")) +
  geom_line(aes(y = logL3, colour = "lambda1 = lambda2")) +
  theme_bw() + labs(
    x = "Pi",
    y = "Log Likelihood Function"
  )
  
```

Here we could see that, $l(\pi, \lambda_1,\lambda_2|Y)=l(1-\pi,\lambda_2,\lambda_1|Y)$. However, they are not the same distribution. We might need other information to differ these two cases.

Then let's try to look at the confidence interval for $\pi$.
Since $\pi$ has the restriction that $0<\pi<1$, we should constrain $\pi$ in a lower dimension parameter space $H$. To test whether $\pi$ should be restricted or not, we would like to perform a likelihood ratio test:
$$H_0: \pi \in \Theta_0$$\
$$H_1:\pi\in\Theta-\Theta_0$$.
The critical value for this test is:
$$2(l(\hat{\theta}, y_{obs})-l(\tilde{\theta}, y_{obs}))\sim\chi^2_{d}$$, since we only reduce one parameter in this case, so d = 1. If the critical value is larger than 3.84, then we will reject the null hypothesis, and conclude that $\pi$ cannot be reduced. However, if the test is not rejected, we will be able to construct the confidence interval for $\pi$. The interval would be $l(\tilde{\theta}, y_{obs})\leq l(\hat{\theta}, y_{obs})+1.92$
```{r question 3 CI for pi}
# Find the 95% confidence interval for pi using inverting likelihood ratio test
pi_CI = function(start, y) {
  pi.grid = seq(0.01, 0.99, 0.01)
  global = em.exponential.mixture(y, start, maxits = 10000, eps = 0.00001)
  n_iter_result = global$iter
  llmax = global[[4]][n_iter_result] # log likelihood for theta hat
  loglik = numeric(length(pi.grid)) # results vector for log likelihood of theta tilde
  
  # Calculate log likelihood function for each fixed pi value
  for (i in 1:length(pi.grid)) {
    temp = em.exponential.mixture(y, start, maxits = 10000, eps = 0.00001, pi.fix = pi.grid[i])
    n_iter = temp[[2]]
    loglik[i] = temp[[4]][n_iter]
  } 
  
  # Calculate the 95% CI for pi
  range(pi.grid[loglik+1.92 >= llmax])
}

# CI for pi
start = list(pi = 0.5, lambda1 = 1, lambda2 = 0.5)
pi_CI(start, data3)

# CI for 1-pi
start2 = list(pi = 0.5, lambda1 = 0.5, lambda2 = 1)
pi_CI(start2, data3)

# CI for pi when lambda1 = lambda2
start3 = list(pi = 0.5, lambda1 = 1, lambda2 = 1)
pi_CI(start3, data3)
```

From the 95% confidence interval, we could be able to do more concret inference. It is quite interesting that the sum of upper bound of $\pi$ and lower bound of $1-\pi$ is 1. Similarly, the lower bound of $\pi$ plus upper bound of $1-\pi$ is also 1. However, if we set $\lambda_1=\lambda_2$, the density function becomes just one exponential distribution density function with only one parameter $\lambda=\lambda_1(\lambda_2)$.

## Question 4
From the question, we know that:
$$f(y_i;\theta)=\pi f_1(y_i;\lambda_1)+(1-\pi)f_2(y_i;\lambda_2)$$.
Then we have:
$$\begin{equation}
\tag{1}
\frac{\partial f}{\partial \pi}=f_1-f_2
\end{equation}$$
$$\begin{equation}
\tag{2}
\frac{\partial f}{\partial \lambda_1}=\pi \frac{\partial f_1}{\partial \lambda_1}
\end{equation}$$
$$\begin{equation}
\tag{3}
\frac{\partial f}{\partial \lambda_2}=(1-\pi) \frac{\partial f_2}{\partial \lambda_2}
\end{equation}$$
Here we also have $$\begin{equation}
\tag{4}
\pi^*=\frac{\pi f_1}{f}
\end{equation}$$. So we have:
$$\begin{equation}
\tag{5}
1-\pi^*=(1-\pi)\frac{\pi f_2}{f}
\end{equation}$$.

#### (a)
$$\begin{align}
\frac{\partial \pi^*}{\partial \pi} &= \frac{f_1f-\pi f_1 \frac{\partial f}{\partial \pi}}{f^2}\\
&=\frac{f_1}{f}-\pi\frac{f_1}{f}\frac{f_1-f_2}{f}\\
&=\frac{\pi^*}{\pi}-\pi\frac{\pi^*}{\pi}(\frac{\pi^*}{\pi}-\frac{1-\pi^*}{1-\pi})\\
&=\frac{\pi^*(1-\pi^*)}{\pi (1-\pi)}
\end{align}$$
$$\begin{align}
\frac{\partial \pi^*}{\partial \lambda_1} &= \frac{\frac{\pi \partial f_1}{\partial \lambda_1}f-\frac{\partial f}{\partial \lambda_1}f1}{f^2}\\
&=\frac{f_1}{f}-\pi\frac{f_1}{f}\frac{f_1-f_2}{f}\\
&=\frac{\pi}{f}(1-\pi) \frac{f_2}{f} \frac{\partial f_1}{\partial \lambda_1}\\
&=\frac{\pi^*}{f_1}(1-\pi^*)\frac{\partial f_1}{\partial \lambda_1}\\
&=\pi^*(1-\pi^*)\frac{\partial}{\partial \lambda_1}logf_1
\end{align}$$
$$\begin{align}
\frac{\partial \pi^*}{\partial \lambda_2} &= -[\frac{\frac{(1-\pi) \partial f_2}{\partial \lambda_2}f-\frac{\partial f}{\partial \lambda_2}(1-\pi)f2}{f^2}]\\
&=-[(1-\pi)\frac{f-(1-\pi)f_2}{f^2}\frac{\partial f_2}{\partial \lambda_2}]\\
&=-[\frac{1-\pi}{f}\pi \frac{f_1}{f}\frac{\partial f_2}{\partial \lambda_2}]\\
&=-[\frac{1-\pi}{f}\pi^*\frac{\partial f_2}{\partial \lambda_2}]\\
&=-\pi^*(1-\pi^*)\frac{\partial}{\partial \lambda_2}logf_2
\end{align}$$

#### (b)
$$\begin{align}
\frac{\partial l_i}{\partial \pi} &= \frac{\partial logf}{\partial \pi}\\
&=\frac{1}{f}\frac{\partial f}{\partial \pi}\\
&=\frac{f_1-f_2}{f}\\
&=\frac{\pi^*}{\pi}-\frac{1-\pi^*}{1-\pi}\\
&=\frac{\pi^*-\pi}{\pi (1-\pi)}
\end{align}$$
$$\begin{align}
\frac{\partial l_i}{\partial \lambda_1} &= \frac{\partial logf}{\partial \lambda_1}\\
&=\frac{1}{f}\frac{\partial f}{\partial \lambda_1}\\
&=\frac{1}{f}\pi \frac{\partial f_1}{\partial \lambda_1}\\
&=\frac{\pi^*}{f_1} \frac{\partial f_1}{\partial \lambda_1}\\
&=\pi^* \frac{\partial logf_1}{\partial \lambda_1}
\end{align}$$
$$\begin{align}
\frac{\partial l_i}{\partial \lambda_2} &= \frac{\partial logf}{\partial \lambda_2}\\
&=\frac{1}{f}(1-\pi)\frac{\partial f_2}{\partial \lambda_2}\\
&=\frac{1-\pi^*}{f_2} \frac{\partial f_2}{\partial \lambda_2}\\
&=(1-\pi^*)\frac{\partial logf_2}{\partial \lambda_2} 
\end{align}$$

#### (c)
$$\begin{align}
\frac{\partial^2 l_i}{\partial \pi^2} &= \frac{\partial \frac{\pi^*(1-\pi^*)}{\pi (1-\pi)}}{\partial \pi}\\
&=\frac{(\frac{\partial \pi^*}{\partial \pi}-1)\pi (1-\pi)-(\pi^*-\pi)(1-2\pi)}{\pi^2(1-\pi)^2}\\
&=\frac{-[\pi^2-2\pi \pi^* + (\pi^*)^2]}{\pi^2(1-\pi)^2}\\
&=-\frac{(\pi^*-\pi)^2}{\pi^2(1-\pi)^2} 
\end{align}$$
$$\begin{align}
\frac{\partial^2 l_i}{\partial \lambda_1 \partial \pi} &= \frac{\partial l_i}{\partial \pi}\frac{\partial logf_1}{\lambda_1}\\
&=-\frac{\pi^*(1-pi^*)}{\pi (1-\pi)}\frac{\partial}{\partial \lambda_2}logf_2 
\end{align}$$
$$\begin{align}
\frac{\partial^2 l_i}{\partial \lambda_1 \partial \pi} &= \frac{\partial \frac{\partial l_i}{\partial \pi}}{\partial \lambda_1}\\
&=\frac{\pi^*(1-pi^*)}{\pi (1-\pi)}\frac{\partial}{\partial \lambda_1}logf_1 
\end{align}$$
$$\begin{align}
\frac{\partial^2 l_i}{\partial \lambda_2 \partial \pi} &= \frac{\partial \frac{\partial l_i}{\partial \pi}}{\partial \lambda_2}\\
&=-\frac{\pi^*(1-pi^*)}{\pi (1-\pi)}\frac{\partial}{\partial \lambda_2}logf_2 
\end{align}$$
$$\begin{align}
\frac{\partial^2 l_i}{\partial \lambda_1^2} &= \frac{\partial \frac{\partial l_i}{\partial \lambda_1}}{\partial \lambda_1}\\
&=\frac{}{}
&=-\frac{\pi^*(1-pi^*)}{\pi (1-\pi)}\frac{\partial}{\partial \lambda_2}logf_2 
\end{align}$$